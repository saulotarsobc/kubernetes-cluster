# Cluster Kubernetes

> _Este reposit√≥rio cont√©m uma documenta√ß√£o detalhada com instru√ß√µes e comandos para configurar, gerenciar e operar um cluster Kubernetes em um ambiente de desenvolvimento local. Inclui desde a configura√ß√£o b√°sica das VMs e rede at√© a implanta√ß√£o de aplica√ß√µes em Kubernetes, integra√ß√£o com ferramentas como Terraform, e acesso ao Kubernetes Dashboard._

Abaixo est√° uma vis√£o geral do conte√∫do abordado:

<div align="center">
  <img src="./image/diagram/diagram.svg" alt="diagram" width="100%" />
</div>

<div align="center">
  <img src="./image/readme/1735746408263.png" alt="virtualbox" width="100%" />
</div>


## üìÑ Vis√£o Geral

- Configura√ß√£o de um cluster Kubernetes em m√°quinas virtuais (VirtualBox).
- Passos para configurar IP est√°tico e hostname das VMs.
- Instala√ß√£o e configura√ß√£o de ferramentas essenciais como Docker, Calico e Kubernetes.
- Cria√ß√£o de deploys e servi√ßos para gerenciar aplica√ß√µes distribu√≠das no cluster.
- Configura√ß√£o do Kubernetes Dashboard para monitoramento e gest√£o.
- Utiliza√ß√£o de Terraform para gerenciar recursos no cluster.

## üñ•Ô∏è Infraestrutura

> O cluster √© composto por tr√™s m√°quinas virtuais rodando Ubuntu Server 24.10, cada uma configurada com 2GB de mem√≥ria e 2 vCPUs:

- **master.k8s.local** - 192.168.1.100
- **worker-1.k8s.local** - 192.168.1.101
- **worker-2.k8s.local** - 192.168.1.102


## üöÄ Recursos Inclu√≠dos

1. **Configura√ß√£o Inicial**:
   - Atualiza√ß√£o de pacotes, instala√ß√£o de ferramentas e configura√ß√£o de IP est√°tico.
   - Desativa√ß√£o de mem√≥ria swap e habilita√ß√£o de encaminhamento de pacotes IPv4.

2. **Instala√ß√£o de Software**:
   - Docker e Containerd.
   - Kubernetes (`kubelet`, `kubeadm`, `kubectl`).

3. **Gerenciamento do Cluster**:
   - Inicializa√ß√£o do cluster com `kubeadm`.
   - Instala√ß√£o de Calico como rede de pods.
   - Implanta√ß√£o de aplica√ß√µes usando Deployments, DaemonSets e Services.

4. **Acesso e Monitoramento**:
   - Configura√ß√£o do Kubernetes Dashboard com autentica√ß√£o baseada em token.
   - Acesso ao cluster a partir de uma m√°quina local utilizando `kubeconfig`.

5. **Automatiza√ß√£o com Terraform**:
   - Cria√ß√£o e gerenciamento de recursos Kubernetes via Terraform.

## üîó Links √öteis

- [Playlist: Kubernetes Cluster no YouTube](https://www.youtube.com/watch?v=iwlNCePWiw4&list=PLHMWRJcYzpI436YPGOf33qOf4p6q8I7pD)

---

> [!WARNING]
> O IP do master.k8s.local deve ser configurado no arquivo de configura√ß√£o do Netplan para 192.168.1.xxx/24.

---

## apt update & upgrade & tools

```bash
sudo apt update
sudo apt upgrade -y
sudo apt install nano inetutils-ping net-tools curl gnupg2 software-properties-common apt-transport-https ca-certificates -y
```

---

## Passos para Configurar o IP Est√°tico:

> Edite o arquivo de configura√ß√£o do Netplan

```bash
ls /etc/netplan/
```

> Normalmente, haver√° um arquivo com extens√£o .yaml, como 01-netcfg.yaml. Abra este arquivo para edi√ß√£o (substitua o nome do arquivo pelo nome listado no seu sistema):

> Modifique o conte√∫do do arquivo para configurar o endere√ßo IP est√°tico. Exemplo de configura√ß√£o para 192.168.1.xxx/24:

> Ap√≥s salvar as altera√ß√µes no arquivo YAML, aplique as novas configura√ß√µes de rede com:

```bash
sudo netplan apply
```

> Confirme se configurou corretamente:

```bash
ip addr show
ping 8.8.8.8
```

---

## Hostnames

> Configurando os Hostnames de cada VM

```bash
sudo hostnamectl set-hostname master.k8s.local # master
sudo hostnamectl set-hostname worker-1.k8s.local # worker-1
sudo hostnamectl set-hostname worker-2.k8s.local # worker-2
```

> Editando o arquivo `/etc/hosts`

```bash
sudo nano /etc/hosts
```

> Adicionando as linhas:

```bash
# k8s cluster nodes
192.168.1.100 master.k8s.local
192.168.1.101 worker-1.k8s.local
192.168.1.102 worker-2.k8s.local
```

![1733442773081](image/readme/1733442773081.png)

> Reinicializando o sistema

```bash
sudo reboot
```

> Verificando os Hostnames

```bash
hostnamectl
```

---

## Desativar a mem√≥ria swap

```bash
sudo swapoff -a
sudo sed -i '/\/swap.img/s/^/#/' /etc/fstab
```

> Ap√≥s executar, voc√™ pode verificar se a altera√ß√£o foi aplicada corretamente com:

```bash
cat /etc/fstab
```

---

## Configurar o encaminhamento de pacotes no IPv4

```bash
sudo modprobe overlay
sudo modprobe br_netfilter
sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sudo sysctl --system
```

---

## Instalar o Docker

```bash
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt update
sudo apt install -y containerd.io
```

```bash
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1
sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd
```

---

## Instala√ßao do kubernetes

```bash
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

---

> [!WARNING]
> Os camandos abaixo devem ser executados no `master.k8s.local`

## Inicializa o cluster

```bash
sudo kubeadm init \
  --apiserver-advertise-address=192.168.1.100 \
  --pod-network-cidr=192.168.0.0/16
```

> Esse pode demorar um pouco. Ele retornar√° os passou seguintes.

> [!WARNING]
> A seguir, apenas um exemplo do que os comandos retornaram nas minhas maquina `master.k8s.local`

![1733482143221](image/readme/1733482143221.png)

> Rode os comando solicitados no `master.k8s.local`

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

> Copie os comandos retornados e execute ele no worker-1.k8s.local e no worker-2.k8s.local

```bash
sudo kubeadm join master.k8s.local:6443 --token cs7vti.**************** \
        --discovery-token-ca-cert-hash sha256:*****************************
```

![1733482487418](image/readme/1733482487418.png)

> Rodando os comando a seguir na maquina `master.k8s.local` j√° da para ver se o cluster foi iniciado corretamente.

```bash
kubectl cluster-info
kubectl get nodes
```

![1733482608410](image/readme/1733482608410.png)

## Instale o Calico Pod Network para na maquina `master.k8s.local`

```bash
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

> Verificaremos se o calico foi implantado com sucesso, verificando os pods no namespacekube-system

```bash
kubectl get pods -n kube-system
```

![1733483417243](image/readme/1733483417243.png)

> Se o status for Em execu√ß√£o, significa que a implanta√ß√£o foi bem-sucedida. Agora, se voc√™ verificar o status dos n√≥s, o status ser√° Pronto.

```bash
kubectl get nodes
```

![1733483448735](image/readme/1733483448735.png)

---

## Deploy com Replica√ß√£o

Aqui est√° um exemplo pr√°tico para rodar um container em cada m√°quina (n√≥) do cluster Kubernetes. Vamos criar uma aplica√ß√£o simples com NGINX, onde cada n√≥ executar√° um pod.

---

### **1. Configura√ß√£o Inicial: Deploy com Replica√ß√£o**

Criaremos um Deployment com 3 r√©plicas, onde cada r√©plica ser√° atribu√≠da automaticamente a um n√≥ do cluster.

```yaml
# arquivo: nginx-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

**Comando para aplicar:**

```bash
kubectl apply -f nginx-deployment.yaml
```

**Verifique se os pods foram criados:**

```bash
kubectl get pods -o wide
```

Isso mostrar√° a localiza√ß√£o dos pods nos n√≥s (`NODE`), algo assim:

```
NAME                                READY   STATUS    RESTARTS   AGE   NODE
nginx-deployment-5d9b85749f-abcde   1/1     Running   0          10s   worker-1.k8s.local
nginx-deployment-5d9b85749f-xyz01   1/1     Running   0          10s   worker-2.k8s.local
nginx-deployment-5d9b85749f-qwert   1/1     Running   0          10s   master.k8s.local
```

---

### **2. Garantindo 1 Pod por M√°quina: DaemonSet**

Se quiser garantir que sempre haver√° um pod em cada n√≥ (independentemente de quantos n√≥s existirem), voc√™ pode usar um DaemonSet.

```yaml
# arquivo: nginx-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

**Comando para aplicar:**

```bash
kubectl apply -f nginx-daemonset.yaml
```

**Verifique os pods criados:**

```bash
kubectl get pods -o wide
```

Voc√™ ver√° que h√° exatamente um pod rodando em cada n√≥, algo assim:

```
NAME                          READY   STATUS    RESTARTS   AGE   NODE
nginx-daemonset-abc12         1/1     Running   0          10s   master.k8s.local
nginx-daemonset-xyz34         1/1     Running   0          10s   worker-1.k8s.local
nginx-daemonset-qwert56       1/1     Running   0          10s   worker-2.k8s.local
```

---

### **3. Expondo a Aplica√ß√£o (Opcional)**

Para acessar os pods em uma aplica√ß√£o distribu√≠da, use um `Service` que balanceia o tr√°fego entre os pods.

```yaml
# arquivo: nginx-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: NodePort
```

**Comando para aplicar:**

```bash
kubectl apply -f nginx-service.yaml
```

**Obtenha o NodePort e acesse a aplica√ß√£o:**

```bash
kubectl get svc nginx-service
```

Voc√™ ver√° algo como:

```
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
nginx-service   NodePort    10.96.183.239    <none>        80:32000/TCP   10s
```

Agora voc√™ pode acessar a aplica√ß√£o em qualquer n√≥, utilizando o IP do n√≥ e a porta `32000`. Por exemplo:

```
http://192.168.1.100:32000  # Acessando o master
http://192.168.1.101:32000  # Acessando o worker-1
http://192.168.1.102:32000  # Acessando o worker-2
```

---

Aqui est√° uma vers√£o passo a passo para configurar o acesso ao **Kubernetes Dashboard** e gerar o token manualmente, garantindo que tudo seja configurado corretamente.

---

### **Passo 1: Implantar o Kubernetes Dashboard**

Aplique o manifesto oficial do Kubernetes Dashboard:

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
```

Este comando cria o Dashboard no namespace `kubernetes-dashboard`. Para verificar:

```bash
kubectl get pods -n kubernetes-dashboard
```

---

## **Kubernetes Dashboard**

### **Passo 2: Expor o Dashboard**

Edite o servi√ßo do Dashboard para torn√°-lo acess√≠vel externamente usando um `NodePort`:

```bash
kubectl -n kubernetes-dashboard edit service kubernetes-dashboard
```

Altere o tipo de servi√ßo de `ClusterIP` para `NodePort`:

```yaml
spec:
  type: NodePort
```

Salve e saia do editor. Em seguida, descubra a porta externa:

```bash
kubectl -n kubernetes-dashboard get svc kubernetes-dashboard
```

Voc√™ ver√° algo como:

```
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   10.96.183.239   <none>        443:32000/TCP   2m
```

Agora voc√™ pode acessar o Dashboard na porta mostrada com o comando anterior(`https://<IP_DO_N√ì>:32000`).

![1735747438705](image/readme/1735747438705.png)

---

### **Passo 3: Criar o Usu√°rio Admin**

Crie um arquivo YAML chamado `dashboard-admin.yaml` com o conte√∫do abaixo para criar um usu√°rio `admin-user` e vincul√°-lo ao `ClusterRoleBinding` com permiss√µes de administrador:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
```

Aplique o arquivo:

```bash
kubectl apply -f dashboard-admin.yaml
```

---

### **Passo 4: Gerar o Token Manualmente**

Com o usu√°rio criado, gere um token para autentica√ß√£o:

```bash
kubectl -n kubernetes-dashboard create token admin-user
```

O comando retorna o token diretamente:

```
eyJhbGciOiJSUzI1NiIsImtpZCI6...
```

Copie o token para usar na interface do Kubernetes Dashboard.

---

### **Passo 5: Fazer Login no Dashboard**

1. Abra o navegador e acesse o Dashboard:

   ```
   https://<IP_DO_N√ì>:<PORTA_NODEPORT>
   ```

   Exemplo:

   ```
   https://192.168.1.100:32000
   ```
2. No painel de login, selecione **Token**.
3. Cole o token gerado no campo e clique em "Login".

---

### **Passo 6: (Opcional) Verificar os Recursos**

Depois de fazer login, voc√™ pode verificar o status dos n√≥s, pods, servi√ßos e outros recursos no Dashboard.

---

### **Nota Adicional**

Se preferir restringir o acesso ou usar certificados para o Dashboard, voc√™ pode configurar o servi√ßo como `LoadBalancer` e usar um balanceador de carga externo ou gerenciar o tr√°fego com um ingress.

---

## **Acessando o Cluster Kubernetes**

Para acessar o cluster Kubernetes a partir da sua m√°quina local (laptop), voc√™ precisa configurar o arquivo `kubeconfig` da sua m√°quina para apontar para o cluster. Aqui est√° o passo a passo detalhado:

---

# **Configura√ß√£o do Kubeconfig**

### **Passo 1: Copiar o Arquivo Kubeconfig do Master**

No n√≥ master do Kubernetes, o arquivo de configura√ß√£o est√° localizado em `/etc/kubernetes/admin.conf`. Este arquivo √© necess√°rio para autenticar e se conectar ao cluster.

1. No n√≥ master, copie o arquivo `admin.conf` para um local tempor√°rio (ou diretamente para sua m√°quina local):

```bash
sudo cat /etc/kubernetes/admin.conf
```

2. Copie o conte√∫do exibido e salve-o no seu laptop como `~/.kube/config`. Certifique-se de criar o diret√≥rio `~/.kube` caso ele n√£o exista:

```bash
mkdir -p ~/.kube
nano ~/.kube/config
```

3. Cole o conte√∫do do arquivo `admin.conf` na janela do editor e salve.

---

### **Passo 2: Tornar o Arquivo Kubeconfig Acess√≠vel**

Altere as permiss√µes do arquivo no laptop para garantir que o Kubernetes CLI (kubectl) possa us√°-lo:

```bash
chmod 600 ~/.kube/config
```

> [!WARNING]
> Pode ser que seja alterar o IP da m√°quina master no arquivo para possibilidade de acesso remoto.

![1733487593079](image/readme/1733487593079.png)

---

### **Passo 3: Verificar Conectividade com o Cluster**

Certifique-se de que sua m√°quina local consegue acessar os n√≥s do cluster Kubernetes. Isso pode exigir configura√ß√£o de rede ou VPN dependendo do ambiente.

1. Teste o acesso aos n√≥s do cluster:

   ```bash
   ping 192.168.1.100
   ping 192.168.1.101
   ping 192.168.1.102
   ```
2. Confirme o acesso ao cluster com `kubectl`:

   ```bash
   kubectl get nodes
   ```

   Se o cluster estiver configurado corretamente, voc√™ ver√° os n√≥s listados.

---

### **Passo 4: Configurar o Acesso Remoto**

Se voc√™ n√£o consegue acessar os IPs diretamente, voc√™ pode configurar um t√∫nel SSH ou expor o servidor API do Kubernetes para acesso externo.

#### **Op√ß√£o 1: T√∫nel SSH**

Estabele√ßa um t√∫nel SSH para o n√≥ master:

```bash
ssh -L 6443:127.0.0.1:6443 user@192.168.1.100
```

Depois, edite o arquivo `~/.kube/config` no laptop para alterar o `server` do cluster para `127.0.0.1:6443`:

```yaml
clusters:
- cluster:
    server: https://127.0.0.1:6443
```

#### **Op√ß√£o 2: Expor a API Kubernetes para o IP P√∫blico**

Edite o arquivo `/etc/kubernetes/manifests/kube-apiserver.yaml` no n√≥ master para permitir o acesso da sua m√°quina local.

1. Abra o arquivo:

   ```bash
   sudo nano /etc/kubernetes/manifests/kube-apiserver.yaml
   ```
2. Adicione o seguinte par√¢metro sob `command`:

   ```yaml
   - --bind-address=0.0.0.0
   ```
3. Reinicie o n√≥ master ou o processo do Kubernetes API Server:

   ```bash
   sudo systemctl restart kubelet
   ```
4. Use o IP do n√≥ master no campo `server` no arquivo kubeconfig no laptop:

   ```yaml
   clusters:
   - cluster:
       server: https://192.168.1.100:6443
   ```

---

### **Passo 5: Configurar Certificados para Seguran√ßa**

Se voc√™ estiver acessando o cluster fora da rede local, considere configurar certificados e firewall para proteger o acesso.

- Gere um certificado para o servidor API do Kubernetes.
- Restrinja o acesso por IP ao n√≥ master usando `iptables` ou configura√ß√µes do firewall.

---

# Terraform

Gerenciar o cluster Kubernetes com **Terraform** √© uma √≥tima maneira de automatizar e padronizar as opera√ß√µes. O Terraform pode ser usado para interagir com o cluster e provisionar recursos como Namespaces, Deployments, Services, ConfigMaps, entre outros. Aqui est√° como voc√™ pode come√ßar:

---

### **Passo 1: Instalar o Provider do Kubernetes**

Certifique-se de ter o Terraform instalado e adicione o provider do Kubernetes ao seu arquivo `main.tf`. O provider do Kubernetes permite que voc√™ gerencie recursos do cluster diretamente.

```hcl
terraform {
  required_providers {
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.21"
    }
  }
}

provider "kubernetes" {
  config_path    = "~/.kube/config" # Caminho para o arquivo kubeconfig
  config_context = "kubernetes-admin@kubernetes" # Contexto do cluster
}
```

- **`config_path`:** Especifica o caminho para o kubeconfig. Normalmente, est√° localizado em `~/.kube/config`.
- **`config_context`:** Define o contexto para o cluster. Substitua pelo contexto correto, caso voc√™ tenha v√°rios clusters configurados.

---

### **Passo 2: Criar Recursos no Kubernetes**

Depois de configurar o provider, voc√™ pode criar recursos Kubernetes como Namespaces, Pods, Deployments, Services, etc.

#### Exemplo: Criar um Namespace

```hcl
resource "kubernetes_namespace" "example" {
  metadata {
    name = "example-namespace"
  }
}
```

#### Exemplo: Criar um Deployment

```hcl
resource "kubernetes_deployment" "nginx" {
  metadata {
    name      = "nginx-deployment"
    namespace = kubernetes_namespace.example.metadata[0].name
    labels = {
      app = "nginx"
    }
  }

  spec {
    replicas = 2

    selector {
      match_labels = {
        app = "nginx"
      }
    }

    template {
      metadata {
        labels = {
          app = "nginx"
        }
      }

      spec {
        container {
          name  = "nginx"
          image = "nginx:latest"

          port {
            container_port = 80
          }
        }
      }
    }
  }
}
```

#### Exemplo: Criar um Service

```hcl
resource "kubernetes_service" "nginx" {
  metadata {
    name      = "nginx-service"
    namespace = kubernetes_namespace.example.metadata[0].name
  }

  spec {
    selector = {
      app = "nginx"
    }

    port {
      port        = 80
      target_port = 80
    }

    type = "NodePort"
  }
}
```

---

### **Passo 3: Aplicar o Terraform**

Execute os comandos abaixo para aplicar a configura√ß√£o no cluster Kubernetes:

1. **Inicializar o Terraform:**

   ```bash
   terraform init
   ```
2. **Planejar as mudan√ßas:**

   ```bash
   terraform plan
   ```
3. **Aplicar as mudan√ßas:**

   ```bash
   terraform apply
   ```

---

### **Passo 4: Validar os Recursos no Cluster**

Depois que o Terraform aplicar as configura√ß√µes, voc√™ pode validar os recursos no cluster Kubernetes:

```bash
kubectl get all -n example-namespace
```

---

### **Passo 5: Gerenciar Altera√ß√µes**

O Terraform permite que voc√™ gerencie altera√ß√µes no cluster declarativamente. Sempre que voc√™ modificar os arquivos `.tf`, o Terraform calcular√° a diferen√ßa e aplicar√° as altera√ß√µes necess√°rias.

---

### **Passo 6: Organizar os Arquivos Terraform**

Para um projeto maior, voc√™ pode organizar seus arquivos Terraform em m√≥dulos ou diret√≥rios para facilitar a manuten√ß√£o:

- `main.tf`: Cont√©m a configura√ß√£o principal.
- `variables.tf`: Define as vari√°veis reutiliz√°veis.
- `outputs.tf`: Define as sa√≠das para exibi√ß√£o.
- `modules/`: Cont√©m m√≥dulos separados para diferentes componentes, como `namespace`, `deployment`, etc.

---

### **Passo 7: Adicionar Integra√ß√µes (Opcional)**

- **Provisionamento de Infraestrutura:** Use Terraform para provisionar os n√≥s do cluster em provedores como DigitalOcean, AWS ou Google Cloud.
- **Integra√ß√£o com CI/CD:** Configure pipelines para aplicar configura√ß√µes Terraform automaticamente.
- **Configura√ß√£o de Secrets:** Combine o Terraform com o HashiCorp Vault ou outras ferramentas de gerenciamento de secrets.

---

## Rodar PODS na M√°quina Master?

> Por padr√£o, no Kubernetes, o n√≥ **master** (tamb√©m chamado de **control-plane**) n√£o √© usado para agendar e executar pods de workloads (trabalhos) de usu√°rios. Isso ocorre devido a um r√≥tulo (`taint`) especial que o Kubernetes aplica ao n√≥ master durante a inicializa√ß√£o do cluster. O objetivo dessa configura√ß√£o padr√£o √© separar as fun√ß√µes do plano de controle e as workloads para garantir:

1. **Estabilidade do Cluster:**

   - O master hospeda componentes cr√≠ticos do Kubernetes, como o API Server, Scheduler, e Controller Manager. Manter esses componentes separados de workloads reduz a chance de interfer√™ncia ou sobrecarga que poderia afetar o gerenciamento do cluster.
2. **Seguran√ßa:**

   - O n√≥ master geralmente tem acesso privilegiado e √© um ponto central no cluster. Separ√°-lo dos workloads limita o escopo de poss√≠veis falhas de seguran√ßa.
3. **Alta Disponibilidade:**

   - Em um cluster de produ√ß√£o com m√∫ltiplos masters, o uso exclusivo dos masters para o plano de controle ajuda a manter o cluster operacional mesmo que alguns n√≥s de trabalho (workers) falhem.

---

### **Como Isso Funciona?**

O Kubernetes usa um mecanismo chamado **taints** para evitar que o n√≥ master seja usado para workloads.

Se voc√™ executar o comando abaixo, ver√° os taints aplicados ao n√≥ master:

```bash
kubectl describe node master.k8s.local
```

Na sa√≠da, voc√™ ver√° algo como:

```
Taints:
  node-role.kubernetes.io/control-plane:NoSchedule
```

Esse `taint` impede que pods comuns sejam agendados no n√≥ master. O `NoSchedule` significa que os pods s√≥ ser√£o agendados no n√≥ se tiverem a toler√¢ncia expl√≠cita para este taint.

---

### **Como Permitir que o Master Execute Pods?**

Se voc√™ deseja usar o n√≥ master para executar pods de workloads, voc√™ pode remover o taint aplicado ao n√≥ master. Aqui est√° como fazer isso:

#### Passo 1: Remover o Taint no N√≥ Master

Execute o comando abaixo para remover o taint:

```bash
kubectl taint nodes master.k8s.local node-role.kubernetes.io/control-plane:NoSchedule-
```

**Explica√ß√£o do comando:**

- O `-` no final remove o taint.
- Substitua `master.k8s.local` pelo nome exato do n√≥ master no seu cluster.

#### Passo 2: Verificar se o Taint foi Removido

Depois de remover o taint, verifique novamente a configura√ß√£o do n√≥:

```bash
kubectl describe node master.k8s.local
```

Na se√ß√£o `Taints`, voc√™ n√£o ver√° mais o `NoSchedule`.

#### Passo 3: Testar o Agendamento de Pods no Master

Agora, se voc√™ criar pods ou deployments sem especificar n√≥s espec√≠ficos, o agendador do Kubernetes poder√° escolher o master para executar os pods.

Exemplo:

```bash
kubectl run test-pod --image=nginx --restart=Never
kubectl get pods -o wide
```

Voc√™ ver√° que o pod pode ser agendado no n√≥ master.

---

### **Considera√ß√µes Importantes**

1. **Desempenho e Estabilidade:**

   - Usar o n√≥ master para workloads pode afetar o desempenho do cluster se os recursos forem limitados, especialmente se houver muitas workloads competindo com os componentes do plano de controle.
2. **Clusters de Produ√ß√£o:**

   - Em ambientes de produ√ß√£o, recomenda-se manter o master dedicado ao controle do cluster. Para pequenos clusters de desenvolvimento ou teste, usar o master para workloads pode ser aceit√°vel.
3. **Escalabilidade:**

   - Se o master for usado para workloads em clusters maiores, √© importante monitorar os recursos (CPU, mem√≥ria, etc.) do n√≥ master para evitar sobrecarga.

---

## **Reverter a Configura√ß√£o para Impedir que o Master Execute Pods**

Para reverter a configura√ß√£o e impedir que o n√≥ master do Kubernetes agende pods de workloads novamente, voc√™ pode adicionar de volta o **taint** ao n√≥ master. Aqui est√° como fazer isso:

---

### **Passo 1: Adicionar o Taint ao N√≥ Master**

Execute o comando abaixo para aplicar novamente o taint padr√£o ao n√≥ master:

```bash
kubectl taint nodes master.k8s.local node-role.kubernetes.io/control-plane:NoSchedule
```

**Explica√ß√£o do comando:**

- **`node-role.kubernetes.io/control-plane`:** √â a chave do taint aplicada ao n√≥ master.
- **`NoSchedule`:** Indica que o n√≥ n√£o pode agendar pods que n√£o tenham a toler√¢ncia expl√≠cita para este taint.

---

### **Passo 2: Verificar se o Taint Foi Aplicado**

Depois de aplicar o taint, verifique se ele foi adicionado corretamente:

```bash
kubectl describe node master.k8s.local
```

Na sa√≠da, na se√ß√£o `Taints`, voc√™ ver√° algo como:

```
Taints:
  node-role.kubernetes.io/control-plane:NoSchedule
```

Isso significa que o master est√° novamente configurado para n√£o agendar pods de workloads.

---

### **Passo 3: Remover Pods que J√° Est√£o Rodando no Master**

Se houver pods de workloads j√° rodando no n√≥ master, voc√™ precisar√° remov√™-los ou realoc√°-los para os n√≥s workers. Siga os passos abaixo:

#### Listar os Pods no Master:

```bash
kubectl get pods -o wide --all-namespaces | grep master.k8s.local
```

#### Excluir os Pods:

Se os pods forem descart√°veis (por exemplo, pods de testes), voc√™ pode exclu√≠-los com o comando:

```bash
kubectl delete pod <nome-do-pod> -n <namespace>
```

Substitua `<nome-do-pod>` e `<namespace>` pelos valores corretos.

#### Reagendar Pods Necess√°rios:

Se os pods precisarem ser mantidos, voc√™ pode criar novos `Deployments` ou `DaemonSets` sem toler√¢ncias para o taint do master. Isso for√ßar√° o Kubernetes a agend√°-los apenas nos n√≥s workers.

---

### **Passo 4: Certifique-se de que o Master Est√° Apenas Controlando o Cluster**

Ap√≥s aplicar o taint e remover os pods de workloads, o master ser√° usado exclusivamente para gerenciar o cluster, como no estado inicial.

---

### **Restaurar Configura√ß√µes Originais (Opcional)**

Se voc√™ quiser garantir que o master esteja exatamente como estava ap√≥s a configura√ß√£o inicial, pode verificar os arquivos do plano de controle, como `kube-apiserver.yaml`, `kube-controller-manager.yaml` e `kube-scheduler.yaml`, localizados em:

```bash
/etc/kubernetes/manifests/
```

Certifique-se de que nenhuma configura√ß√£o adicional foi alterada.

---
